{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvrk10d9YLLK8HLeHh/Cv2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hari-19/Regression_Neural_Networks/blob/master/Regression_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YPlqpM-RsTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This Code uses L-1 Layers of Sigmoid and 1 layer of Linear Function.\n",
        "\n",
        "#The ReLU function is been commented to act as a Linear function, Since, the dataset doesn't require a ReLU function.\n",
        "\n",
        "#The Cost is computed using Squared Error function."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTEzLDLLYrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ssUL1pebAV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Set needs to be Loaded\n",
        "\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuCChUVaeTA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "# uploaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GDwMU2deYa6",
        "colab_type": "code",
        "outputId": "cc5b0821-61da-46e7-915c-fdc5ce78faf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/hari-19/Regression_Neural_Networks/master/AirQualityUCI.csv\"\n",
        "# df = pd.read_csv(io.BytesIO(uploaded[url]))\n",
        "df = pd.read_csv(url)\n",
        "print(df)"
      ],
      "execution_count": 580,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      CO(GT)  PT08.S1(CO)  NMHC(GT)  ...      AH  Unnamed: 13  Unnamed: 14\n",
            "0        2.6         1360       150  ...  0.7578          NaN          NaN\n",
            "1        2.0         1292       112  ...  0.7255          NaN          NaN\n",
            "2        2.2         1402        88  ...  0.7502          NaN          NaN\n",
            "3        2.2         1376        80  ...  0.7867          NaN          NaN\n",
            "4        1.6         1272        51  ...  0.7888          NaN          NaN\n",
            "...      ...          ...       ...  ...     ...          ...          ...\n",
            "9352     3.1         1314      -200  ...  0.7568          NaN          NaN\n",
            "9353     2.4         1163      -200  ...  0.7119          NaN          NaN\n",
            "9354     2.4         1142      -200  ...  0.6406          NaN          NaN\n",
            "9355     2.1         1003      -200  ...  0.5139          NaN          NaN\n",
            "9356     2.2         1071      -200  ...  0.5028          NaN          NaN\n",
            "\n",
            "[9357 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gLJRuwphrVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfNumpy = df.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nHYRclihrbL",
        "colab_type": "code",
        "outputId": "2f1975b3-e030-419c-a48b-f7d766093e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "dfNumpy"
      ],
      "execution_count": 582,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.600e+00,  1.360e+03,  1.500e+02, ...,  7.578e-01,        nan,\n",
              "               nan],\n",
              "       [ 2.000e+00,  1.292e+03,  1.120e+02, ...,  7.255e-01,        nan,\n",
              "               nan],\n",
              "       [ 2.200e+00,  1.402e+03,  8.800e+01, ...,  7.502e-01,        nan,\n",
              "               nan],\n",
              "       ...,\n",
              "       [ 2.400e+00,  1.142e+03, -2.000e+02, ...,  6.406e-01,        nan,\n",
              "               nan],\n",
              "       [ 2.100e+00,  1.003e+03, -2.000e+02, ...,  5.139e-01,        nan,\n",
              "               nan],\n",
              "       [ 2.200e+00,  1.071e+03, -2.000e+02, ...,  5.028e-01,        nan,\n",
              "               nan]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 582
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgVVPVOahreB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dfNumpy[:,0:12]\n",
        "X = np.transpose(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbU9NPJphrg2",
        "colab_type": "code",
        "outputId": "fa5f6fe8-06b4-4871-d04e-f72b99569100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "print(X)\n",
        "print(np.shape(X))"
      ],
      "execution_count": 584,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   2.6    2.     2.2 ...    2.4    2.1    2.2]\n",
            " [1360.  1292.  1402.  ... 1142.  1003.  1071. ]\n",
            " [ 150.   112.    88.  ... -200.  -200.  -200. ]\n",
            " ...\n",
            " [1268.   972.  1074.  ... 1092.   770.   816. ]\n",
            " [  13.6   13.3   11.9 ...   26.9   28.3   28.5]\n",
            " [  48.9   47.7   54.  ...   18.3   13.5   13.1]]\n",
            "(12, 9357)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOPr7IEkNfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = dfNumpy[:,12:13]\n",
        "Y = np.transpose(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiYesgUwkNqE",
        "colab_type": "code",
        "outputId": "90b39564-78f7-42a3-8792-6c7d7381b4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Y)\n",
        "print(np.shape(Y))"
      ],
      "execution_count": 586,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.7578 0.7255 0.7502 ... 0.6406 0.5139 0.5028]]\n",
            "(1, 9357)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W-lhE9FNJ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns tanh of Data\n",
        "\n",
        "def tanh(data):\n",
        "  data = np.tanh(data)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vVmAGTKOXN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns ReLU of Data\n",
        "\n",
        "def relu(data):\n",
        "    # data\n",
        "    # data = np.where(data>0,data,0)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHaipuzfpK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Returns Sigmoid of Data\n",
        "\n",
        "def sigmoid(data):\n",
        "    data = 1 + np.exp(-data)\n",
        "    data = 1/data\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bkNO610pu9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ReLU Derivative\n",
        "def reluDerivative(x):\n",
        "  # print(\"########################################\")\n",
        "  # print(\"X before :\")\n",
        "  # print(x)\n",
        "  # print(np.shape(x))\n",
        "  # print(\"X after\")\n",
        "  # x[x<=0] = 0\n",
        "  # x[x>0] = 1\n",
        "\n",
        "  x.fill(1)\n",
        "  # print(x)\n",
        "  # print(np.shape(x))\n",
        "  # print(\"#########################################\")\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-4C4As7r9Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ReLU Backwards\n",
        "def relu_backward(dA, cache):\n",
        "  # print(\"ReLU\")\n",
        "  # print(cache)\n",
        "  # print(np.shape(cache))\n",
        "  # print(\"calling ReLU derivative\")\n",
        "  # reluDerivative(cache)\n",
        "  # print(cache)\n",
        "  dZ = np.multiply(dA, reluDerivative(cache))\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Wtt0RPpvHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sigmoid Derivative\n",
        "def sigmoidDerivative(x):\n",
        "  # print(\"%%%SigmoidDerivative%%\")\n",
        "  # print(\"X\")\n",
        "  # print(x)\n",
        "  tempX = sigmoid(x)\n",
        "  x = np.multiply(tempX, (1 - tempX))\n",
        "  # print(x)\n",
        "  # print(\"$$$$$$$$$$$$$$\")\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gbw6cKkqtbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sigmoid Backwards\n",
        "def sigmoid_backward(dA, cache):\n",
        "  # print(\"^^^^^^^^^^^^\")\n",
        "  # print(\"Sigmoid\")\n",
        "  # print(dA)\n",
        "  dZ = np.multiply(dA, sigmoidDerivative(cache)) \n",
        "  # print(dZ)\n",
        "  # print(\"^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUavT2SQ7zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameter initialization\n",
        "\n",
        "    # Arguments:\n",
        "    # layer_dims -- python array (list) containing the dimensions of each layer in the network\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #                 Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #                 bl -- bias vector of shape (layer_dims[l], 1)\n",
        "\n",
        "def initialize_parameters(layer_dims):\n",
        "    parameters={}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p10QIQ1Kn0Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Arguments:\n",
        "    # layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #                 Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #                 bl -- bias vector of shape (layer_dims[l], 1)\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.rand(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQjmONqKeBXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The linear part of a layer's forward propagation.\n",
        "\n",
        "    # Arguments:\n",
        "    # A -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    # Returns:\n",
        "    # Z -- the input of the activation function\n",
        "    # cache -- a python tuple containing \"A\", \"W\" and \"b\"\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssn2XNtefec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    # Arguments:\n",
        "    # A_prev -- activations from previous layer : (size of previous layer, number of examples)\n",
        "    # W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    # b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\"(will be coded later)\n",
        "\n",
        "    # Returns:\n",
        "    # A -- the output of the activation function\n",
        "    # cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "   \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # print(\"A##########\")\n",
        "        # print(A_prev)\n",
        "        # print(\"W#########\")\n",
        "        # print(W)\n",
        "        # print(\"B########\")\n",
        "        # print(b)\n",
        "        # print(\"##########\")\n",
        "        # print(np.shape(A_prev))\n",
        "        # print(np.shape(W))\n",
        "        # print(np.shape(b))\n",
        "        # print(np.dot(W,A_prev))\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A = relu(Z)\n",
        "        # print(\"#######Forward#######\")\n",
        "        # print(Z)\n",
        "        # print()\n",
        "        # print(A)\n",
        "        # print(\"#####################\")\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPz2XqBhf-ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    # Forward propagation for the [Linear --> Sigmoid] * (L-1) -> [LINEAR->ReLU] computation\n",
        "    \n",
        "    # Arguments:\n",
        "    # X -- data, numpy array of shape (input size, number of examples)\n",
        "    # parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    # Returns:\n",
        "    # AL -- last post-activation value\n",
        "    # caches -- list of caches containing:\n",
        "    #             every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = int(len(parameters)/2)                 # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "    # print(\"L_model_forward  parameters\")\n",
        "    # print(parameters)\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"relu\")\n",
        "\n",
        "    # print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
        "    # print(\"L_model_forward AL\")\n",
        "    # print(AL)\n",
        "    # print(np.count_nonzero(AL))\n",
        "\n",
        "    # print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cKeHjAPH-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #Cost Function\n",
        "    #Squared Error\n",
        "\n",
        "    # Arguments:\n",
        "    # AL -- probability vector corresponding to predictions, shape (1, number of examples)\n",
        "    # Y -- true value\n",
        "\n",
        "    # Returns:\n",
        "    # cost -- cross-entropy cost\n",
        "\n",
        "def compute_cost(AL, Y):  \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    cost = np.mean(np.square(Y - AL) )\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spWMPF7lPzCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The Linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    # Arguments:\n",
        "    # dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    # cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    # Returns:\n",
        "    # dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    # dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    # db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m) * np.dot(dZ, np.transpose(A_prev))\n",
        "    db = (1/m) * np.sum(dZ,axis = 1,keepdims = True)\n",
        "    dA_prev = np.dot(np.transpose(W), dZ)\n",
        "   \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxF0SH4cTJ8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # The backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    # Arguments:\n",
        "    # dA -- post-activation gradient for current layer l \n",
        "    # cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    # activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    # Returns:\n",
        "    # dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    # dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    # db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "   \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTC-9c3HTceE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The backward propagation for the [LINEAR->Sigmoid] * (L-1) -> [LINEAR -> ReLU] group\n",
        "    \n",
        "#     Arguments:\n",
        "#     AL -- Output of the forward propagation (L_model_forward())\n",
        "#     Y -- True Value\n",
        "#     caches -- list of caches containing:\n",
        "\n",
        "    \n",
        "#     Returns:\n",
        "#     grads -- A dictionary with the gradients\n",
        "#              grads[\"dA\" + str(l)] = ... \n",
        "#              grads[\"dW\" + str(l)] = ...\n",
        "#              grads[\"db\" + str(l)] = ... \n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = -(Y - AL) # derivative of cost with respect to AL\n",
        "    \n",
        "    # Lth layer (ReLU -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"relu\")\n",
        "    \n",
        "    # print(\"##########L_model_bk\")\n",
        "    # print(grads[\"dA\" + str(L-1)])\n",
        "    # print(grads[\"dW\" + str(L)])\n",
        "    # print(grads[\"db\" + str(L)])\n",
        "    # print(\"%%%%%%%%%%%%%%%%%%%%%%%\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (Sigmoid -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-TK-cumYfUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Update parameters using gradient descent\n",
        "    \n",
        "    # Arguments:\n",
        "    # parameters -- python dictionary containing your parameters \n",
        "    # grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- python dictionary containing your updated parameters \n",
        "    #               parameters[\"W\" + str(l)] = ... \n",
        "    #               parameters[\"b\" + str(l)] = ...\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = int(len(parameters)/2)  # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - ( learning_rate * grads[\"dW\" + str(l+1)] ) \n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - ( learning_rate * grads[\"db\" + str(l+1)] ) \n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOKcXToAmhCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CONSTANTS ###\n",
        "layers_dims = [12, 10, 7, 5, 1] #  4-layer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-dbuwQZm2ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    # Arguments:\n",
        "    # X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    # Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    # layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    # learning_rate -- learning rate of the gradient descent update rule\n",
        "    # num_iterations -- number of iterations of the optimization loop\n",
        "    # print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    # Returns:\n",
        "    # parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.009, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (≈ 1 line of code)\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # print(\"%%%%%%%%%%%%%%%%%%\")\n",
        "        # print(X)\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # print(\"&&&&&&&&&&&&&&&&\")\n",
        "        # print(AL)\n",
        "        # print(\"&&&&&&&&&&&&&&&&\")\n",
        "\n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW6Ja2dTnM-v",
        "colab_type": "code",
        "outputId": "e31c5905-af0f-4d41-ac78-af4992f60997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "parameters = L_layer_model(X, Y, layers_dims, num_iterations = 2500, print_cost = True)"
      ],
      "execution_count": 612,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 1566.007507\n",
            "Cost after iteration 100: 1500.955479\n",
            "Cost after iteration 200: 1220.985789\n",
            "Cost after iteration 300: 869.290789\n",
            "Cost after iteration 400: 618.428668\n",
            "Cost after iteration 500: 441.051334\n",
            "Cost after iteration 600: 314.974943\n",
            "Cost after iteration 700: 225.087664\n",
            "Cost after iteration 800: 160.911760\n",
            "Cost after iteration 900: 115.063392\n",
            "Cost after iteration 1000: 82.298284\n",
            "Cost after iteration 1100: 58.879089\n",
            "Cost after iteration 1200: 42.138346\n",
            "Cost after iteration 1300: 30.170823\n",
            "Cost after iteration 1400: 21.615201\n",
            "Cost after iteration 1500: 15.498583\n",
            "Cost after iteration 1600: 11.125576\n",
            "Cost after iteration 1700: 7.999096\n",
            "Cost after iteration 1800: 5.763794\n",
            "Cost after iteration 1900: 4.165632\n",
            "Cost after iteration 2000: 3.022995\n",
            "Cost after iteration 2100: 2.206039\n",
            "Cost after iteration 2200: 1.621934\n",
            "Cost after iteration 2300: 1.204312\n",
            "Cost after iteration 2400: 0.905719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfGiLYvmnU_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpbM5td2vh0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}